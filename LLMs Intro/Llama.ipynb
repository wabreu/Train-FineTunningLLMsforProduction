{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download model\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\tmodel_id,\n",
    "\ttrust_remote_code=True,\n",
    "\ttorch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate answer\n",
    "prompt = \"Translate English to French: Configuration files are easy to use!\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "# print answer\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Falcon\n",
    "model_id = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "##Dolly\n",
    "model_id = \"databricks/dolly-v2-3b\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
